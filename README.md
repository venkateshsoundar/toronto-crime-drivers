# Dynamics of Crime in Toronto: Socioeconomic and Environmental Drivers

![Data Source: City of Toronto Open Data](https://img.shields.io/badge/Data-TorontoOpenData-blue)
![License: MIT](https://img.shields.io/badge/License-MIT-green)

## ğŸ“– Project Overview
This repository hosts a Jupyter Notebook (`DATA_604_L01_05_Final_Report.ipynb`) that investigates the dynamics of crime across Toronto neighbourhoods. By integrating multiple datasetsâ€”police budgets, crime rates, household income, education levels, shelter occupancy, and bike rack locationsâ€”we explore how socioeconomic and environmental factors contribute to crime trends.

## ğŸ‘¥ Authors
- Aaron Gelfand
- David Griffin
- Jackson Meier
- Steen Rasmussen
- Venkateshharan Balu Soundararajan

## ğŸ” Guiding Questions
1. **Police Budget & Crime Trends**: How does the annual operating budget for police services relate to neighbourhood crime rates?
2. **Household Income & Crime Rates**: What is the association between mean household income and crime statistics?
3. **Education Level & Crime**: How do neighbourhood education attainment levels correlate with crime incidents?
4. **Shelter Occupancy & Crime**: Does shelter occupancy influence crime patterns at the neighbourhood level?
5. **Bike Racks & Bike Thefts**: How does the presence of bike parking infrastructure affect bike theft occurrences?

## ğŸ—„ï¸ Data Sources
Place the following CSV files in a `data/` folder at the project root:
- **Police Budget**: `Gross Operating Budget.csv` (converted to `converted_budget.csv`)
- **Crime Rates**: `neighbourhood-crime-rates.csv` (converted to `converted_crime.csv`)
- **Income & Education**: `neighbourhood-profiles-2021-158-model.csv`
- **Shelter Occupancy**: `Data_Shelter_Occupancy_Merged.csv`
- **Geospatial Mapping**: `Address Points_Neighbourhoods.csv`  
  _Note: This file is very large (~1.8â€¯GB). We recommend downloading only the neighbourhood subset or hosting it externally and loading via URL, rather than including it directly in the repo._
- **Bike Racks**: `Bicycle Parking Racks Data - 4326.csv` (cleaned as `Cleaned_Bicycle_Parking_Data.csv`)

Additional intermediate CSVs generated by the notebook (e.g., `crime_long2.csv`, `capacity_query.csv`, `bike_theft_area.csv`) are stored in `data/` after preprocessing.

## ğŸ“ Uploading Data Files to Git
To include your data files in the repository and push them to remote GitHub:

1. **Add files**
   ```bash
   git add data/Address\ Points_Neighbourhoods.csv \
           data/Bicycle\ Parking\ Racks\ Data\ -\ 4326.csv \
           data/Data_Shelter_Occupancy_Merged.csv \
           data/Gross\ Operating\ Budget.csv \
           data/neighbourhood-crime-rates.geojson \
           data/neighbourhood-profiles-2021-158-model.csv
   ```
   Or to add all files in the `data/` folder:
   ```bash
   git add data/*
   ```

2. **Commit changes**
   ```bash
   git commit -m "Add raw data files for analysis"
   ```

3. **Push to remote**
   ```bash
   git push origin main
   ```

### Handling Very Large Files
For files like `Address Points_Neighbourhoods.csv` (~1.8â€¯GB), consider one of these approaches:

#### a) Use Git Large File Storage (LFS)
```bash
git lfs install
git lfs track "data/Address Buttons_Neighbourhoods.csv"
git add .gitattributes
git add "data/Address Points_Neighbourhoods.csv"
git commit -m "Add Address Points file using Git LFS"
git push origin main
```

#### b) Host Externally and Download Programmatically
```python
import requests

url = "https://your-bucket.s3.amazonaws.com/Address%20Points_Neighbourhoods.csv"
response = requests.get(url, stream=True)
with open("data/Address Points_Neighbourhoods.csv", "wb") as f:
    for chunk in response.iter_content(chunk_size=8192):
        f.write(chunk)
```

## âš™ï¸ Dependencies
Create a Python (>=3.8) virtual environment and install required packages:

```bash
pip install -r requirements.txt
```

**requirements.txt**:
```
pandas
numpy
sqlalchemy
mysql-connector-python
matplotlib
seaborn
statsmodels
scipy
geopandas
shapely
folium
plotly
```

## ğŸ’¾ Database Setup
The notebook uploads cleaned tables into a MySQL database. Update the connection string in the first code cell:
```python
engine = create_engine(
    "mysql+mysqlconnector://<user>:<password>@<host>:<port>/<database>"
)
```

## ğŸš€ Usage
1. Clone the repository:
   ```bash
   git clone https://github.com/<username>/<repo-name>.git
   cd <repo-name>
   ```
2. Place all raw and cleaned CSVs into `data/`.
3. Activate your virtual environment and install dependencies.
4. Open Jupyter Notebook and run:
   ```bash
   jupyter notebook DATA_604_L01_05_Final_Report.ipynb
   ```
5. Execute cells sequentially to reproduce data cleaning, analysis, and visualizations.

## ğŸ“‘ Notebook Structure
1. **Setup & Imports**: Load Python libraries and configure database engine.
2. **Data Cleaning & Loading**: Read raw CSVs, clean variables, and push to MySQL.
3. **Exploratory Analysis**: Visualize individual relationships for each guiding question.
4. **Geospatial Mapping**: Create neighbourhood maps with crime and infrastructure overlays.
5. **Statistical Modeling**: Fit regression and correlation models to quantify associations.
6. **Discussion & Conclusions**: Summarize key findings and policy implications.

## ğŸ¤ Contributing
Contributions and improvements are welcome:
1. Fork this repository.
2. Create a branch: `git checkout -b feature-name`.
3. Commit your changes: `git commit -m "Add feature"`.
4. Push: `git push origin feature-name`.
5. Open a Pull Request for review.

## ğŸ“œ License
This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.

---
*Prepared by DATA 604 Group L01-05*
